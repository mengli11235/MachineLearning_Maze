from easy_maze import MazeSimulator
from RL_brain import SarsaLambdaTable
import time
import pandas as pd

def learning(epi, time_in_ms, _is_render, RL, env):
    for episode in range(epi):
        # initial observation
        agent = env.reset()

        # RL choose action based on observation
        action = RL.choose_action(str(agent))

        # initial all zero eligibility trace
        RL.eligibility_trace *= 0

        while True:
            # fresh env
            env.render(time_in_ms)

            # RL take action and get next observation and reward
            agent_, reward, is_done = env.step(action)

            # RL choose action based on next observation
            action_ = RL.choose_action(str(agent_))

            # RL learn from this transition (s, a, r, s, a) ==> Sarsa
            RL.learn(str(agent), action, reward, str(agent_), action_, is_done)

            # swap observation and action
            agent = agent_
            action = action_

            # break while loop when end of this episode
            if is_done:
                break

    # end of game
    print('game over')
    if _is_render:
        time.sleep(1)
        env.destroy()
    env.destroy()

    RL.q_table.to_csv("temp_q_table.csv", sep=',', encoding='utf-8')
    print(RL.q_table)

def running(epi, time_in_ms, _is_render, RL, env):
    try:
        df = pd.DataFrame.from_csv('temp_q_table.csv', sep=',', encoding='utf8')
        RL.set_prior_qtable(df)
        print("set prior q")
    except Exception:
        pass

    for episode in range(epi):
        # initiate the agent
        agent = env.reset()
        
        # RL choose action based on observation
        action = RL.choose_action(str(agent))

        # initial all zero eligibility trace
        RL.eligibility_trace *= 0

        while True:
            # fresh env
            env.render(time_in_ms)

            # RL take action and get next observation and reward
            agent_, reward, is_done = env.step(action)

            # RL choose action based on next observation
            action_ = RL.choose_action(str(agent_))

            # RL learn from this transition (s, a, r, s, a) ==> Sarsa
            RL.learn(str(agent), action, reward, str(agent_), action_, is_done)

            # break while loop when end of this episode
            if is_done:
                break

    # end of game
    print('game over')
    if _is_render:
        time.sleep(1)
        env.destroy()



if __name__ == "__main__":
    # set if render the GUI
    is_render = False
    # set number of runs
    episodes = 1000
    # animation interval
    interval = 0.005
    # set the size of maze: column x row
    size_maze = [4, 6]
    # initial position of the agent
    # all position count from 0
    init_pos = [0, 0]
    
    # initiate maze simulator for learning and running
    maze = MazeSimulator(size_maze[1], size_maze[0], init_pos, is_render)
    demo_maze = MazeSimulator(size_maze[1], size_maze[0], init_pos, True)

    # set fixed object ([column, row], reward, isFinishedWhenReach)
    # set rewards
    # maze.set_fixed_obj([3, 4], 1, True)
    demo_maze.set_fixed_obj([3, 4], 1, True)
    # maze.set_fixed_obj([1, 3], 1, True)
    # demo_maze.set_fixed_obj([1, 3], 1, True)
    # maze.set_collect_all_rewards([[3, 4], [1, 3]], 1, "golds")
    # demo_maze.set_collect_all_rewards([[3, 4], [1, 3]], 1, "golds")

    # set traps
    # maze.set_fixed_obj([1, 2], -1, True)
    demo_maze.set_fixed_obj([1, 2], -1, True)
    # maze.set_fixed_obj([2, 1], -1, True)
    demo_maze.set_fixed_obj([2, 1], -1, True)

    # build the rendered maze
    maze.build_maze()
    demo_maze.build_maze()

    # initiate SarsaLearner
    actions = list(range(maze.n_actions))
    learning_rate = 0.01
    reward_gamma = 0.9
    greedy = 0.7
    SLearner = SarsaLambdaTable(actions, learning_rate, reward_gamma, greedy)

    # run the simulation of training
    if is_render:
        maze.after(1, learning(episodes, interval, is_render, SLearner, maze))
        maze.mainloop()
    else:
        learning(episodes, interval, is_render, SLearner, maze)

    # Q decision with 99% greedy strategy
    #demo_greedy = 0.99
    #demo_interval = 0.2
    #SRunner = SarsaLambdaTable(actions, learning_rate, reward_gamma, demo_greedy)
    #running(50, demo_interval, True, SRunner, demo_maze)
    
    #env = MazeSimulator()
    #RL = SarsaLambdaTable(actions=list(range(env.n_actions)))

    #env.after(100, learning)
    #env.mainloop()
